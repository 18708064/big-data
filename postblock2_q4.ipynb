{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHj0zQvM6O+pHXQorBLM/t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/18708064/big-data/blob/master/postblock2_q4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3.1"
      ],
      "metadata": {
        "id": "C00HqWCpGTvp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtSbXYd4sI01"
      },
      "outputs": [],
      "source": [
        "# Map function\n",
        "def map(filename, content):\n",
        "    # For each character in the content of the file\n",
        "    for char in content:\n",
        "        # Emit the character as the key and the value as 1\n",
        "        EmitIntermediate(char, 1)\n",
        "\n",
        "# Reduce function\n",
        "def reduce(char, values):\n",
        "    # Sum the counts for each character key\n",
        "    result = sum(values)\n",
        "    Emit(char, result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3.5"
      ],
      "metadata": {
        "id": "-o6RAPvjFkyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combiner function\n",
        "def combiner(char, values):\n",
        "    # Partially sum values before sending to reducer\n",
        "    partial_sum = sum(values)\n",
        "    Emit(char, partial_sum)\n"
      ],
      "metadata": {
        "id": "Eq-IyzYUFhxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4"
      ],
      "metadata": {
        "id": "g_y3oegKFrbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mapper(hostname, document):\n",
        "    terms = document.split()\n",
        "    term_counts = Counter(terms)\n",
        "    for term, count in term_counts.items():\n",
        "        if count > 1:\n",
        "            yield (hostname, (term, count))\n",
        "\n",
        "\n",
        "def reducer(hostname, term_vectors):\n",
        "    term_frequency = Counter()\n",
        "\n",
        "    # Sum up term frequencies\n",
        "    for term, count in term_vectors:\n",
        "        term_frequency[term] += count\n",
        "\n",
        "    # Emit terms that occur at least twice\n",
        "    filtered_term_vector = {term: count for term, count in term_frequency.items() if count >= 2}\n",
        "\n",
        "    yield (hostname, filtered_term_vector)\n"
      ],
      "metadata": {
        "id": "eH7uK0rqFuQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4.2\n"
      ],
      "metadata": {
        "id": "LgMGH-qnSq_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import groupby\n",
        "from operator import itemgetter\n",
        "import re\n",
        "import nltk\n",
        "from collections import Counter  # Import Counter for counting term frequencies\n",
        "\n",
        "# Download the stopwords from nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lul1OSlIWPKZ",
        "outputId": "a9713a2c-5967-41ea-92f3-740a15c2b9dd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Mapper function\n",
        "def mapper(hostname, document):\n",
        "    terms = document.split()  # Split the document into words (tokens)\n",
        "    term_counts = Counter(terms)  # Count the occurrences of each term\n",
        "\n",
        "    # Emit each term and its count\n",
        "    for term, count in term_counts.items():\n",
        "        if count > 1:  # Only emit terms that occur more than once\n",
        "            yield (hostname, (term, count))\n",
        "\n",
        "# Reducer function\n",
        "def reducer(hostname, term_vectors):\n",
        "    term_frequency = Counter()  # Initialize counter to store term frequencies\n",
        "\n",
        "    # Aggregate the term frequencies\n",
        "    for term, count in term_vectors:\n",
        "        term_frequency[term] += count\n",
        "\n",
        "    # Filter and emit terms that occur at least twice\n",
        "    filtered_term_vector = {term: count for term, count in term_frequency.items() if count >= 2}\n",
        "\n",
        "    yield (hostname, filtered_term_vector)\n",
        "\n",
        "# Cleaner function to process lines and filter stopwords\n",
        "def cleaner(line):\n",
        "    # Lowercase all words, keep only alphabetical characters, and allow apostrophes\n",
        "    words = re.findall(r'[a-z\\']+', line.lower())\n",
        "\n",
        "    for word in words:\n",
        "        # Omit apostrophe 's' (assuming users won't type them in a search)\n",
        "        word = word.replace(\"'\", '')\n",
        "\n",
        "        # Check if the word is empty or a stopword\n",
        "        if word != '' and word not in stopwords.words('english'):\n",
        "            yield word\n",
        "\n",
        "# Function to sort intermediate results by key\n",
        "def intermediate_sort(data):\n",
        "    \"\"\"\n",
        "    Collect by key.\n",
        "    \"\"\"\n",
        "    data = sorted(data)\n",
        "    return [(k, list(tuple(zip(*g))[1])) for k, g in groupby(data, itemgetter(0))]\n",
        "\n",
        "# Function to run the MapReduce process\n",
        "def run(sources_dict):\n",
        "    \"\"\"\n",
        "    Simulate the MapReduce process with map and reduce steps.\n",
        "    :param sources_dict: Dictionary of document IDs and file paths.\n",
        "    \"\"\"\n",
        "    map_result = []\n",
        "    reduce_result = []\n",
        "\n",
        "    # Apply the map function to each document\n",
        "    for k, v in sources_dict.items():\n",
        "        # Read the document and apply the map function\n",
        "        with open(v, 'r') as f:\n",
        "            map_result += list(mapper(k, f.read()))\n",
        "\n",
        "    # Sort and group the map results by key\n",
        "    intermediate_result = intermediate_sort(map_result)\n",
        "\n",
        "    # Apply the reduce function to each key\n",
        "    for elem in intermediate_result:\n",
        "        reduce_result.append(list(reducer(elem[0], elem[1])))\n",
        "\n",
        "    return map_result, intermediate_result, reduce_result\n"
      ],
      "metadata": {
        "id": "0LBTgNTCSta9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shell Commands (for file setup):"
      ],
      "metadata": {
        "id": "yeNOTSm1Yz3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p input/\n",
        "!echo 'the cat sat on the mat' > input/d1.txt\n",
        "!echo 'the dog sat on the log' > input/d2.txt\n"
      ],
      "metadata": {
        "id": "tNpjX58RXxAR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example to run and display results\n",
        "\n",
        "map_result, intermediate_result, reduce_result = run({'D1': 'input/d1.txt', 'D2': 'input/d2.txt'})\n",
        "\n",
        "# Print the results\n",
        "print(\"Map Result:\")\n",
        "print(map_result)\n",
        "\n",
        "print(\"\\nIntermediate Result:\")\n",
        "print(intermediate_result)\n",
        "\n",
        "print(\"\\nReduce Result:\")\n",
        "print(reduce_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unKeaz4MX3BI",
        "outputId": "5fdc6fdc-4d75-4357-e103-a10ba4d5aa7e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Map Result:\n",
            "[('D1', ('the', 2)), ('D2', ('the', 2))]\n",
            "\n",
            "Intermediate Result:\n",
            "[('D1', [('the', 2)]), ('D2', [('the', 2)])]\n",
            "\n",
            "Reduce Result:\n",
            "[[('D1', {'the': 2})], [('D2', {'the': 2})]]\n"
          ]
        }
      ]
    }
  ]
}