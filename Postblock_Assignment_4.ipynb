{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMk7pZVTW9+/kGVv1+A/xF2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/18708064/big-data/blob/master/Postblock_Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction**\n",
        "\n",
        "\n",
        "This notebook demonstrates the implementation of the MapReduce paradigm to generate term vectors from documents. The goal is to explore how to use the mapper and reducer functions to extract words from multiple documents and compute their frequencies. The MapReduce model is commonly used for distributed computing and data aggregation in big data applications.\n",
        "\n",
        "In this task, we simulate the process by applying MapReduce locally, processing multiple documents hosted on different websites. Each document contains text, and our job is to extract words and determine how frequently each word appears within a given document. Additionally, the reducer function will filter out words that appear less than twice, ensuring only frequently occurring words are included in the final result."
      ],
      "metadata": {
        "id": "mNVhYaVLlT3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import groupby"
      ],
      "metadata": {
        "id": "4IwokXenu9xX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Helper function to sort and group intermediate data by key\n",
        "def intermediate_sort(data):\n",
        "    data = sorted(data)\n",
        "    return [(k, [v for _, v in g]) for k, g in groupby(data, key=lambda x: x[0])]\n"
      ],
      "metadata": {
        "id": "lC2_R9wrmDdY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapper function\n",
        "def mapper(hostname, document_content):\n",
        "    words = document_content.split()\n",
        "    for word in words:\n",
        "        yield (hostname, word), 1\n",
        "\n",
        "# Reducer function with filtering for frequency >= 2\n",
        "def reducer(key, values):\n",
        "    total_count = sum(values)\n",
        "    if total_count >= 2:\n",
        "        yield key, total_count\n"
      ],
      "metadata": {
        "id": "KJnoOx2_tHFJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(sources_dict):\n",
        "    map_result = []\n",
        "    reduce_result = []\n",
        "\n",
        "    # Run the mapper on each document\n",
        "    for hostname, content in sources_dict.items():\n",
        "        map_result.extend(list(mapper(hostname, content)))\n",
        "\n",
        "    # Sort and group the intermediate results\n",
        "    intermediate_result = intermediate_sort(map_result)\n",
        "\n",
        "    # Run the reducer\n",
        "    for elem in intermediate_result:\n",
        "        reduce_result.extend(list(reducer(elem[0], elem[1])))\n",
        "\n",
        "    return map_result, intermediate_result, reduce_result\n"
      ],
      "metadata": {
        "id": "JyAm9XxLtZ1W"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example Input Data"
      ],
      "metadata": {
        "id": "-AqCf4nQthFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example input: documents hosted on different sites\n",
        "documents = {\n",
        "    'www.site1.com': 'apple banana apple',\n",
        "    'www.site2.com': 'banana orange apple',\n",
        "    'www.site3.com': 'orange banana banana'\n",
        "}\n"
      ],
      "metadata": {
        "id": "srK5yz8AtjRu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the MapReduce process\n",
        "_, _, result = run(documents)\n",
        "\n",
        "# Display the final results\n",
        "print(\"Final Results:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhD6fKYwtnTn",
        "outputId": "472478b3-0826-4f5d-a65b-d469e8bb1c53"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Results: [(('www.site1.com', 'apple'), 2), (('www.site3.com', 'banana'), 2)]\n"
          ]
        }
      ]
    }
  ]
}